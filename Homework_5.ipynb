{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Group 19\n",
    "## Visit the Wikipedia hyperlinks graph!\n",
    "\n",
    "**Group's members:** Francesco Russo, Marco Vicentini & Sara Cordaro\n",
    "\n",
    "\n",
    "### Research questions 1\n",
    "\n",
    "\n",
    "At the beginning we imported all the libraries and declared the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "# Hyperlink network of Wikipedia\n",
    "hyperlink = 'wiki-topcats-reduced.txt'\n",
    "# Which articles are in which of the top categories\n",
    "categories = 'wiki-topcats-categories.txt'\n",
    "# Names of the articles\n",
    "articles = 'wiki-topcats-page-names.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we worked with the file *wiki-topcats-page-names.txt*, that associate an index to each article.\n",
    "\n",
    "We decided to create a dictionary that has the article's id as key and the article's name as value. Then we converted the dictionary in a dataframe to check if the names of each article is correct, without escape or other particular characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with article's id as key and article's name as value\n",
    "idx_name = {}\n",
    "with open(articles) as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(' ', 1)\n",
    "        idx_name[int(key)] = val.strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary in datframe\n",
    "df_articles = pd.DataFrame.from_dict(idx_name, orient='index', columns = ['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chiasmal syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kleroterion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pinakion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LyndonHochschildSerre spectral sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zariski's main theorem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name\n",
       "0                        Chiasmal syndrome\n",
       "1                              Kleroterion\n",
       "2                                 Pinakion\n",
       "3  LyndonHochschildSerre spectral sequence\n",
       "4                   Zariski's main theorem"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our **graph**. \n",
    "\n",
    "We worked with the file *wiki-topcats-reduced.txt*, that show us the edges of each vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with article's id as key and list of article's hyperlinks as value\n",
    "# so we can take only the nodes requested\n",
    "graph = defaultdict(list)\n",
    "with open(hyperlink) as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split('\\t')\n",
    "        graph[int(key)].append(int(val.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *vertices_n* is the number of all articles find in the first dictionary *df_articles*, **1791489**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791489\n"
     ]
    }
   ],
   "source": [
    "# number of vertices\n",
    "vertices_n = len(idx_name.keys())\n",
    "print(vertices_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *nodes_wc* is the number of vertices pf the *graph* that has at least a outgoing edge, **428957**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428957\n"
     ]
    }
   ],
   "source": [
    "# number of vertices without some categories removed from the graph\n",
    "nodes_wc = len(graph.keys())\n",
    "print(nodes_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the library *networkx* to try to plot the graph, but it is too big.\n",
    "\n",
    "\n",
    "In these code's lines we computed the total number of edges **2645247**, saved in the variable *edges_n*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize directed graph\n",
    "graph_nx = nx.DiGraph()\n",
    "graph_nx.add_nodes_from(list(graph.keys()))\n",
    "# add edges\n",
    "edges_n = 0\n",
    "for k in graph.keys():\n",
    "    edges_n += len(graph[k])\n",
    "    for j in graph[k]:\n",
    "        graph_nx.add_edge(k,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645247\n"
     ]
    }
   ],
   "source": [
    "# number of edges\n",
    "print(edges_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28511807 is the edges's number of the total graph, considering all the categories.\n",
    "\n",
    "\n",
    "To be more precise we calculated the total number of nodes in the graph, including the nodes with only incident edges.\n",
    "We found **461193** vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all number of vertices without some categories removed from the graph \n",
    "# also the nodes that has only incident edges\n",
    "all_nodes = list(graph.keys())\n",
    "for k in graph.keys():\n",
    "    all_nodes += graph[k]\n",
    "all_nodes = set(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes_wc = len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461193\n"
     ]
    }
   ],
   "source": [
    "print(all_nodes_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check if the graph is directed or undirected, so we studied the graph. \n",
    "\n",
    "- We iterate all the nodes;\n",
    "- We take the node *A*;\n",
    "- We take the list of vertices, connected to the node *A* that we are checking;\n",
    "- We take all the nodes in the list and we take the node *B*; \n",
    "- We check if the node *A* is present in the list of vertices, connected to the node *B*;\n",
    "- If we find just a node *B* that doesn't have the node *A* in its list, we can say that the graph is directed, so we stop every loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph is directed\n"
     ]
    }
   ],
   "source": [
    "# check if graph is directed or not\n",
    "directed = False\n",
    "for k in graph.keys():\n",
    "    for j in graph[k]:\n",
    "        if k not in graph[j]:\n",
    "            directed = True\n",
    "        break\n",
    "    break\n",
    "if directed == True:\n",
    "    print('The graph is directed')\n",
    "else:\n",
    "    print('The graph is undirected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this function says that the graph is **directed**.\n",
    "\n",
    "\n",
    "At this point we check if the graph is **dense** or **sparse**.\n",
    "\n",
    "A **dense** graph is a graph in which the number of edges is close to the maximal number of edges. The opposite, a graph with only a few edges, is a **sparse** graph. The distinction between sparse and dense graphs is rather vague, and depends on the context.\n",
    "\n",
    "\n",
    "For **undirected simple** graphs, the graph density is defined as:\n",
    "\n",
    "$D={\\frac {2|E|}{|V|\\,(|V|-1)}}$\n",
    "\n",
    "For **directed simple** graphs, the graph density is defined as:\n",
    "\n",
    "$D={\\frac {|E|}{|V|\\,(|V|-1)}}$\n",
    "\n",
    "where *E* is the number of edges and *V* is the number of vertices in the graph. The maximum number of edges for an undirected graph is $|V|(|V|-1)/2$, so the *maximal density* is 1 (for complete graphs) and the *minimal density* is 0. The maximum number of edges for a directed graph is $|V|(|V|-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum number of edges\n",
    "max_edges_directed = abs(all_nodes_wc)*(abs(all_nodes_wc) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212698522056\n"
     ]
    }
   ],
   "source": [
    "print(max_edges_directed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph density for a directed simple graphs\n",
    "density_direct = (abs(edges_n))/max_edges_directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2436602635647606e-05\n"
     ]
    }
   ],
   "source": [
    "print(density_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is **sparse**, because the density is 1.2436602635647606e-05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handshaking Lemma** : $\\sum _{v\\in V}\\deg(v)=2|E|\\,$ total number of edges, if the graph is full.\n",
    "\n",
    "Now to compute the degree of each vertex, we create a new dictionary *idx_degree*,  has the node as key and a list of incident edges as value. So we could count the number of incident edges for every node.\n",
    "\n",
    "We found that the **average degree** is **7.503863632495362**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to compute the degree of each vertex\n",
    "# idx_degree has the node as key and a list of incident edges as value\n",
    "idx_degree = defaultdict(list)\n",
    "for k in graph.keys():\n",
    "    for j in graph[k]:\n",
    "        idx_degree[j].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average node degree\n",
    "average_degree = 0\n",
    "for k in idx_degree.keys():\n",
    "    average_degree += len(idx_degree[k])\n",
    "average_degree = average_degree/len(idx_degree.keys())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.503863632495362\n"
     ]
    }
   ],
   "source": [
    "print(average_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the graph\n",
    "# nx.draw_spectral(graph,with_labels = True,node_color = 'lightskyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research questions 2\n",
    "\n",
    "For the second research we worked with the file *wiki-topcats-categories.txt*, so we created a dictionary *category_art* with article's category as key and list of article's idx as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with article's category as key and list of article's idx as value\n",
    "category_art = {}\n",
    "count = 0\n",
    "with open(categories) as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        (key, val) = line.split(';')\n",
    "        if val.strip() == '':\n",
    "            category_art[key.replace('Category:','')] = []\n",
    "        else:\n",
    "            list_art = (val.strip()).split(' ')\n",
    "            category_art[key.replace('Category:','')] = list(map(int,list_art))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17364\n"
     ]
    }
   ],
   "source": [
    "print(len(category_art.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we took only the category with at least *3500* article, then we cleaned the dictionary and we took only the nodes belonging to the graph that we are studying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dictionary without some categories removed from the graph\n",
    "category_clean = defaultdict(list)\n",
    "for k in category_art.keys():\n",
    "    # we take the categoy only if the list has at least 3500 category\n",
    "    if len(category_art[k]) >= 3500:\n",
    "        for node in category_art[k]:\n",
    "            if node in all_nodes:\n",
    "                category_clean[k].append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number and list of the categories that we have to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "# list of all categories\n",
    "all_categories = list(category_clean.keys())\n",
    "print(len(all_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English_footballers', 'The_Football_League_players', 'Association_football_forwards', 'Association_football_goalkeepers', 'Association_football_midfielders', 'Association_football_defenders', 'Living_people', 'Year_of_birth_unknown', 'Harvard_University_alumni', 'Major_League_Baseball_pitchers', 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 'Indian_films', 'Year_of_death_missing', 'English_cricketers', 'Year_of_birth_missing_(living_people)', 'Rivers_of_Romania', 'Main_Belt_asteroids', 'Asteroids_named_for_people', 'English-language_albums', 'English_television_actors', 'British_films', 'English-language_films', 'American_films', 'Fellows_of_the_Royal_Society', 'People_from_New_York_City', 'American_Jews', 'American_television_actors', 'American_film_actors', 'Debut_albums', 'Black-and-white_films', 'Year_of_birth_missing', 'Place_of_birth_missing_(living_people)', 'Article_Feedback_Pilot', 'American_military_personnel_of_World_War_II', 'Windows_games']\n"
     ]
    }
   ],
   "source": [
    "print(list(category_clean.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code's lines we ask a category $C_0 =\\{article_1, article_2, \\dots\\}$ as input. Then we ranked all of the nodes of the graph according to the following criteria:\n",
    "\n",
    "\n",
    "$distance(C_0, C_i) = median(ShortestPath(C_0, C_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year_of_birth_unknown\n"
     ]
    }
   ],
   "source": [
    "# take the category 0 as input\n",
    "request_cat = input()\n",
    "# list of all nodes belong to this category 0\n",
    "cat_nodes = category_clean[request_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to give the category **'Year_of_birth_unknown'** as input, because it is one of the smallest list of nodes.\n",
    "\n",
    "\n",
    "In the following code's lines we decided to implement the **Breadth-first search (BFS)**, because it is the algorithm that we thought it could be more efficient and faster than the others. In this way we searched all the shortest paths between each node of the category *'Year_of_birth_unknown'* and all the nodes of the graph. We saved these informations in the dictionary *len_path* that has a graph's node as key and the list of all the distances between the node and all the nodes in the requested category. The distance from the same node it is **0** and, if a node doesn't arrive to the other node, the distance is **infinite**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary len_path for key a node as value a dictionary\n",
    "len_path = {}\n",
    "for node in all_nodes:\n",
    "    # this dictionary has as key the starting node and as value a list of all the distances\n",
    "    len_path[node] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFS(graph, s, len_path):\n",
    "    # initialize the distance\n",
    "    curr_dist = 0\n",
    "    # update len path the distance of node s \n",
    "    len_path[s].append(curr_dist)\n",
    "    # nodes to visit\n",
    "    queue = set(graph[s])\n",
    "    # nodes visited yet\n",
    "    visited = set([s])  \n",
    "    # if queue isn't empty\n",
    "    while queue:\n",
    "        # update curr_dist \n",
    "        curr_dist += 1\n",
    "        temp = []\n",
    "        # iterate nodes to visit\n",
    "        for v in queue:\n",
    "            # update the dictionary of the shortest distance\n",
    "            len_path[v].append(curr_dist)\n",
    "            temp += graph[v]\n",
    "            visited.add(v)\n",
    "        # update queue with only the nodes that we haven't visited yet\n",
    "        queue = (set(temp)).difference(visited)\n",
    "    return len_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate the nodes of the category 'Year_of_birth_unknown'\n",
    "for n in cat_nodes:\n",
    "    # find all the shortest path\n",
    "    len_path = BFS(graph,n,len_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to save the dictionary *len_path* in a file, because of this algorithm spends much time to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "with open('category_shortest_path.pickle','wb') as file:\n",
    "    pickle.dump(len_path,file, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open the file with the dictionary\n",
    "cat_file = open('category_shortest_path.pickle','rb')\n",
    "shortest_path = pickle.load(cat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we computed $distance(C_0, C_i) = median(ShortestPath(C_0, C_i))$ for each category, according to the category 'Year_of_birth_unknown', our $C_0$.\n",
    "\n",
    "To save the informations about the median and then create the list of categories, representing the \n",
    "\n",
    "$block_{RANKING} =\\begin{bmatrix} C_0 \\\\ C_1 \\\\ \\dots \\\\ C_c\\\\ \\end{bmatrix}$ , \n",
    "\n",
    "we created a dictionary *block_ranking* that has a determinate value of the distance as key and a list of tuples as value. The tuple has two element: the first element is the count of the key in the list and the second is the category $C_i$ that have the $distance(C_0, C_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate all categories and order for the distance from C0 'Year_of_birth_unknown'\n",
    "# dictionary that has as key the median of the shortest path and as value a list of the categories\n",
    "block_ranking = defaultdict(list)\n",
    "# number of nodes in requested category\n",
    "cat_nodes = len(category_clean[request_cat])\n",
    "# iterate all the categories\n",
    "for cat in category_clean.keys():\n",
    "    # category requested - median 0\n",
    "    if cat == request_cat:\n",
    "        block_ranking[0].append((0,cat))\n",
    "    # other categories   \n",
    "    else:\n",
    "        # number of nodes in the category - cat\n",
    "        cat_n = len(category_clean[cat])\n",
    "        # length of the list\n",
    "        tot = cat_n * cat_nodes\n",
    "        # intialize list of all the shortest paths of this category - cat\n",
    "        list_path = []\n",
    "        # iterate all the node in category cat\n",
    "        for node in category_clean[cat]:\n",
    "            # update list\n",
    "            list_path += shortest_path[node]\n",
    "        num_inf = tot - (len(list_path))\n",
    "        # if list smaller than what we expect - append the infinite distances\n",
    "        if num_inf != 0:   \n",
    "            list_path += ([np.inf] * (tot - len(list_path)))\n",
    "        if num_inf > tot//2:\n",
    "            key = np.inf\n",
    "            count = num_inf\n",
    "        else:\n",
    "            array_path = np.array(list_path)\n",
    "            # compute the median of this category cat\n",
    "            key = np.median(array_path)\n",
    "            count = list_path.count(key)\n",
    "        value = (count, cat)\n",
    "        # update the block ranking \n",
    "        block_ranking[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {inf: [(10435866, 'English_footballers'), (10553063, 'The_Football_League_players'), (7660553, 'Association_football_forwards'), (6061970, 'Association_football_goalkeepers'), (9284793, 'Association_football_midfielders'), (7285457, 'Association_football_defenders'), (464429820, 'Living_people'), (7631868, 'Harvard_University_alumni'), (7401986, 'Major_League_Baseball_pitchers'), (8379594, 'Year_of_death_missing'), (4729434, 'English_cricketers'), (39753055, 'Year_of_birth_missing_(living_people)'), (24219182, 'Main_Belt_asteroids'), (9770594, 'Asteroids_named_for_people'), (4449794, 'Fellows_of_the_Royal_Society'), (8383590, 'Year_of_birth_missing'), (7255785, 'Place_of_birth_missing_(living_people)'), (5148539, 'American_military_personnel_of_World_War_II'), (5116028, 'Windows_games')], 0: [(0, 'Year_of_birth_unknown')], 7.0: [(3156487, 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies'), (3515163, 'Indian_films'), (1060144, 'English_television_actors'), (1420032, 'British_films'), (5243193, 'American_films'), (912379, 'American_Jews'), (3903883, 'American_television_actors'), (4310842, 'Black-and-white_films'), (731224, 'Article_Feedback_Pilot')], 8.0: [(2920862, 'Rivers_of_Romania'), (977607, 'English-language_albums'), (589444, 'People_from_New_York_City')], 6.0: [(13519140, 'English-language_films'), (8885835, 'American_film_actors')], 9.0: [(805703, 'Debut_albums')]})\n"
     ]
    }
   ],
   "source": [
    "print(block_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All distances found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf, 0, 7.0, 8.0, 6.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "print(list(block_ranking.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we saved in the list *cat_block* all the categories, ordered according to the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the categories, ordered according to the distance\n",
    "cat_block = []\n",
    "for k in sorted(block_ranking.keys()):\n",
    "    sort_block =  sorted(block_ranking[k], key=lambda tup: tup[0])\n",
    "    for cat in sort_block:\n",
    "        cat_block.append(cat[1])\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the $block_{RANKING}$ that we obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year_of_birth_unknown', 'American_film_actors', 'English-language_films', 'Article_Feedback_Pilot', 'American_Jews', 'English_television_actors', 'British_films', 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 'Indian_films', 'American_television_actors', 'Black-and-white_films', 'American_films', 'People_from_New_York_City', 'English-language_albums', 'Rivers_of_Romania', 'Debut_albums', 'Fellows_of_the_Royal_Society', 'English_cricketers', 'Windows_games', 'American_military_personnel_of_World_War_II', 'Association_football_goalkeepers', 'Place_of_birth_missing_(living_people)', 'Association_football_defenders', 'Major_League_Baseball_pitchers', 'Harvard_University_alumni', 'Association_football_forwards', 'Year_of_death_missing', 'Year_of_birth_missing', 'Association_football_midfielders', 'Asteroids_named_for_people', 'English_footballers', 'The_Football_League_players', 'Main_Belt_asteroids', 'Year_of_birth_missing_(living_people)', 'Living_people']\n"
     ]
    }
   ],
   "source": [
    "print(cat_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "\n",
    "We computed subgraph induced by $C_0$ . For each node compute the sum of the weigths of the in-edges.\n",
    "\n",
    "$score_{article_i} = \\sum_{j \\in in-edges(article_i)} w_j$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the score to assign to each node\n",
    "def score_first_category(C,g):\n",
    "    # initialize the dictionary of scores assigned to nodes in category C\n",
    "    global res \n",
    "    res = { i : 0 for i in C }\n",
    "    # for each node in the first category\n",
    "    for node in C:\n",
    "        # for each one of its successors (nodes which are reached by an outgoing edge of the previous one) \n",
    "        for suc in list(g.successors(node)):\n",
    "            # if the successor is in the first category\n",
    "            if suc in C:\n",
    "                # increase its score by one\n",
    "                res[suc]+=1\n",
    "    # update the graph\n",
    "    for n in res:\n",
    "        g.node[n]['weight']=res[n]\n",
    "    # return the updated graph\n",
    "    return g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the weigths to the edges\n",
    "def give_weight_to_edges(C,g):\n",
    "    # for each node in the considered category\n",
    "    for n in C:\n",
    "        # get its score\n",
    "        node_w=g.node[n]['weight']\n",
    "        # give a weight to its outgoing edges, based on the node's score\n",
    "        for e in list(g.out_edges(n)):\n",
    "            source=e[0]\n",
    "            target=e[1]\n",
    "            g[source][target]['weight'] = node_w\n",
    "    # return the updated graph\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to start giving scores to the nodes of the first category \n",
    "res = {}\n",
    "graph_nx_up = score_first_category(category_clean[cat_block[0]],graph_nx)\n",
    "graph_nx_up = give_weight_to_edges(category_clean[cat_block[0]],graph_nx_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we printed the first five nodes's scores of the first category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3335 0\n",
      "10527 0\n",
      "16310 0\n",
      "22286 0\n",
      "23468 1\n"
     ]
    }
   ],
   "source": [
    "for key in list(res.keys())[:5]:\n",
    "    print(key, res[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "\n",
    "In this step we extended the graph to the nodes that belong to $C_1$ . Thus, for each article in $C_1$ we computed the score as before. Now the in-edges coming from the previous category, $C_0$ , have as weights the score of the node that sends the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the scores for the nodes which are not in the first category\n",
    "def score_notfirst_category(C,g):\n",
    "    # compute the scores as in the first category (i.e. taking into account only the nodes in the same category)\n",
    "    g = score_first_category(C,g)\n",
    "    # then, for each node in the category\n",
    "    for n in C:\n",
    "        # for each one of its incoming edges\n",
    "        for i in list(g.in_edges(n)):\n",
    "            # if the edge doesn't come from a node of the same category\n",
    "            if i[0] not in C:\n",
    "                # try to update its score, using the weight of the incoming edge\n",
    "                try:\n",
    "                    g.node[n]['weight']+=g.node[i[0]]['weight']\n",
    "                # catch the possible exception (i.e. the edge has not a weight, because the node from which\n",
    "                # it starts has not been visited yet)\n",
    "                except:\n",
    "                    continue\n",
    "    # return the updated graph\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we give scores to the nodes of the category C1\n",
    "graph_nx_up = score_notfirst_category(category_clean[cat_block[1]],graph_nx_up)\n",
    "graph_nx_up = give_weight_to_edges(category_clean[cat_block[1]],graph_nx_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we printed the firs five nodes's scores of the category $C_1$, then we assigned the weigths to te edges, according to the weigths of the category $C_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 0\n",
      "980 0\n",
      "1088 1\n",
      "1099 0\n",
      "1106 1\n"
     ]
    }
   ],
   "source": [
    "for key in list(res.keys())[:5]:\n",
    "    print(key, res[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "\n",
    "Now we repeated *Step 2* up to the last category of the ranking. In the last step we could clearly see the weight update of the edge coming from node E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [2:14:46<00:00, 2354.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# given it we can procede for all the other category, the important things is that we respect the order of the block_ranking\n",
    "for c in tqdm(cat_block[2:]):\n",
    "    graph_nx_up = score_notfirst_category(category_clean[c],graph_nx_up)\n",
    "    graph_nx_up = give_weight_to_edges(category_clean[c],graph_nx_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize block vector\n",
    "block_vector = list()\n",
    "\n",
    "# for each category \n",
    "for category in cat_block:\n",
    "    # initialize the block of nodes of the category, and turn it into a heap, for faster sorting\n",
    "    block = list()\n",
    "    heapq.heapify(block)\n",
    "    # for each node in the category\n",
    "    for node in category_clean[category]:\n",
    "        # push the tuple (score, node) into the heap\n",
    "        heapq.heappush(block, ((graph_nx_up.node[node]['weight'], node)))\n",
    "    # get the list of (score, node) tuples from the heap, sorted according to the scores\n",
    "    block = heapq.nlargest(len(block), block)\n",
    "    # append it to the block vector\n",
    "    block_vector.append(block)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2629075, 1084068), (1814763, 1639508), (1670602, 158355), (1352982, 1659360), (658894, 1151309), (174077, 1686459), (131765, 1167053), (30420, 1151492), (10402, 1038076), (5044, 1244814), (2345, 233504), (1998, 137662), (1545, 1656777), (704, 1269822), (246, 51844), (220, 1031761), (152, 1113103), (55, 584219), (47, 1659362), (21, 62684), (18, 330031), (15, 528662), (14, 426344), (14, 219346), (13, 1602954), (12, 170163), (11, 836597), (11, 666855), (10, 1656780), (10, 169696), (9, 1656794), (9, 1656455), (9, 672781), (9, 666857), (7, 1343014), (7, 1342864), (7, 170578), (6, 1656778), (6, 1404839), (6, 1404830), (6, 1342960), (6, 666853), (6, 324414), (6, 204079), (6, 78609), (5, 1779656), (5, 1766063), (5, 1203496), (5, 1203235), (5, 1203095), (5, 1109485), (5, 1109348), (5, 672511), (5, 536462), (5, 317555), (5, 174582), (5, 159920), (5, 159730), (5, 159606), (5, 64632), (4, 1765837), (4, 1765831), (4, 1765824), (4, 1602546), (4, 1443739), (4, 1380565), (4, 1344701), (4, 1343206), (4, 1342803), (4, 1340874), (4, 1203101), (4, 1122762), (4, 666879), (4, 666871), (4, 666870), (4, 666859), (4, 655296), (4, 324170), (4, 209046), (4, 185120), (4, 174439), (4, 174427), (4, 170973), (4, 170972), (4, 170971), (4, 170970), (4, 170969), (4, 170158), (4, 168258), (4, 168251), (4, 168145), (4, 168001), (4, 166284), (4, 159750), (4, 62695), (4, 34422), (3, 1779800), (3, 1779795), (3, 1766721), (3, 1766449)]\n"
     ]
    }
   ],
   "source": [
    "print(block_vector[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the final list of ranked articles (coming from the categories, which have also been ranked)\n",
    "final_rank = []\n",
    "# for each index (which corresponds to a category), in the block vector\n",
    "for idx1 in range(len(block_vector)):\n",
    "    # for each index (which corresponds to a (score, node) tuple), inside the category\n",
    "    for idx2 in range(len(block_vector[idx1])):\n",
    "        # get the node from the tuple\n",
    "        node = block_vector[idx1][idx2][1]\n",
    "        # append the name of the article (corresponding to the node) to the final list\n",
    "        final_rank.append(idx_name[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary Jo Pehl', 'James R. Whelan', 'Michael Munn', 'Frankie Ingrassia', 'Kim Jong-un', 'Ricky Vela', 'Caitlin Dulany', 'Jeryl Prescott', 'Pete Lee-Wilson', 'Jon H. Else', 'Francis Salvador', 'Veronica Vera', 'L Bu', 'Anna Lehr', 'Bradley Burston', 'Jerry Booth', 'D. B. Cooper', 'A. M. Rathnam', 'Bridget Hodson', 'Diogenes Lartius', 'Wilma Subra', 'Gus Kallio', 'Hilde Holovsky', 'Gheorghe tefan', 'Lawrence Rosen (anthropologist)', 'Pausanias (geographer)', 'James Adair (serjeant)', 'Stephen Dingate', 'Dong Zhuo', 'Theocritus', 'Yuan Shu', 'Li Linfu', 'Andrew Gibson (footballer)', 'Tom Faulkner', 'Symeon of Durham', 'Penda of Mercia', 'Stobaeus', 'Zhang Fei', 'William Smith (congressman)', 'Ballard Smith', 'Edwin, Earl of Mercia', 'Joe Harris (cricketer)', 'Chief Crow', 'Tim Stevenson', 'Michael Knighton', 'Stigand', 'Ivan Alexander of Bulgaria', 'Amasis II', 'Horemheb', 'Nefertiti', 'Pope Nicholas III', 'Pope Alexander II', 'Chetan (actor)', 'Sidney Stanley', 'Francis White (Virginia)', 'Valerius Maximus', 'Tancred of Hauteville', 'Pope Nicholas II', 'Sweyn II of Denmark', 'Sextus Empiricus', 'Maria Sanudo, Lady of Andros', 'Florence Sanudo', 'Crusino I Sommaripa, Lord of Paros', 'Hashim Khan', 'Joseph N. Langan', 'Kieran White', 'Pope John VIII', 'Gospatric, Earl of Northumbria', 'Wulfhere of Mercia', 'Goscelin', 'Ay', 'Devapala', 'John Cutbush (cricketer)', 'Pye (Sussex cricketer)', 'Joseph Rudd', 'Ridgeway (Sussex cricketer)', 'Kerry Anne Wells', 'One Who Walks With the Stars', 'Margo Guryan', 'Dubhaltach Mac Fhirbhisigh', 'Florus', 'Marcus Licinius Crassus Dives (consul 14 BC)', 'Achilles Tatius', 'Heliodorus of Emesa', 'Xenophon of Ephesus', 'Longus', 'Chariton', 'Hesychius of Alexandria', 'Richard Meighen', 'William Ponsonby (publisher)', 'John Shank', 'Humphrey Moseley', 'Pope Eugene III', 'Pope Benedict VIII', 'Crates of Thebes', 'Sir William Osborne, 8th Baronet', 'Walter of Lorraine', 'William Edington', 'Sabinianus (consul 505)', 'Andronikos Doukas (general under Romanos IV)']\n"
     ]
    }
   ],
   "source": [
    "print(final_rank[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We printed only the first 100 articles in the list *final_rank*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
